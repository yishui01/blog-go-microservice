version: "3"
services:
  mysql:
    image: mysql:${MYSQL_VERSION}
    container_name: mysql
    ports:
      - "${MYSQL_HOST_PORT}:3306"
    volumes:
      - ${MYSQL_CONF_FILE}:/etc/mysql/conf.d/mysql.cnf:ro
      - ${MYSQL_DATA_DIR}:/var/lib/mysql/:rw
      - ${MYSQL_INIT_FILE}:/docker-entrypoint-initdb.d/:rw
    restart: always
    networks:
      - default
    environment:
      MYSQL_ROOT_PASSWORD: "${MYSQL_ROOT_PASSWORD}"
      MYSQL_DATABASE: "${MYSQL_INIT_DB}"
      MYSQL_PORT: "${MYSQL_HOST_PORT}"
      TZ: "$TZ"

  redis:
    image: redis:${REDIS_VERSION}
    container_name: redis
    ports:
      - "${REDIS_HOST_PORT}:6379"
    volumes:
      - ${REDIS_CONF_FILE}:/etc/redis.conf:ro
    restart: always
    entrypoint: ["redis-server", "/etc/redis.conf"]
    environment:
      TZ: "$TZ"
    networks:
      - default

  elasticsearch:
    build:
      context: ./build/docker/elasticsearch
      args:
        ELASTICSEARCH_VERSION: ${ELASTICSEARCH_VERSION}
        ELASTICSEARCH_PLUGINS: ${ELASTICSEARCH_PLUGINS}
    container_name: elasticsearch
    environment:
      - TZ=$TZ
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    volumes:
      - ${ELASTICSEARCH_JVM_OPT}:/usr/share/elasticsearch/config/jvm.options
      - ${ELASTICSEARCH_DATA_DIR}:/usr/share/elasticsearch/data
      - ${ELASTICSEARCH_CONF_FILE}:/usr/share/elasticsearch/elasticsearch.yml
    hostname: elasticsearch
    restart: always
    ports:
      - "${ELASTICSEARCH_HOST_PORT_C}:9200"
      - "${ELASTICSEARCH_HOST_PORT_S}:9300"


  kibana:
    image: kibana:${KIBANA_VERSION}
    container_name: kibana
    environment:
      TZ: "$TZ"
      elasticsearch.hosts: http://elasticsearch:9200
    volumes:
      - ${KIBANA_CONF_FILE}:/usr/share/kibana/config/kibana.yml
    hostname: kibana
    depends_on:
      - elasticsearch
    restart: always
    ports:
      - "${KIBANA_HOST}:5601"


  logstash:
    image: logstash:${LOGSTASH_VERSION}
    container_name: logstash
    hostname: logstash
    restart: always
    depends_on:
      - elasticsearch
    volumes:
      - ${LOGSTASH_JVM_OPT}:/usr/share/logstash/config/jvm.options
      - ${LOGSTASH_CONF_FILE}:/usr/share/logstash/config/logstash.yml
      - ${LOGSTASH_PIPELINE_DIR}:/usr/share/logstash/pipeline
    environment:
      TZ: "$TZ"
      LS_JAVA_OPTS: "-Xmx256m -Xms256m"
    ports:
      - "${LOGSTASH_HOST_PORT_C}:9600"
      - "${LOGSTASH_HOST_PORT_S}:5044"


  jaeger:
    image: jaegertracing/all-in-one:${JAEGER_VERSION}
    container_name: jaeger
    environment:
      - COLLECTOR_ZIPKIN_HTTP_PORT=9411
      - ES_SERVER_URLS=http://elasticsearch:9200
      - SPAN_STORAGE_TYPE=elasticsearch
      - ES_TAGS_AS_FIELDS=true
    restart: always
    networks:
      - default
    expose:
      - "5775"
      - "6831"
      - "6832"
      - "5778"
      - "16686"
      - "14268"
      - "14250"
      - "9411"
    ports:
      - "5775:5755/udp"
      - "6831:6831/udp"
      - "6832:6832/udp"
      - "5778:5778"
      - "16686:16686"
      - "14268:14268"
      - "14250:14250"
      - "9411:9411"


  etcd0:
    image: daocloud.io/daocloud/etcd:${ETCD_VERSION}
    container_name: etcd0
    restart: always
    volumes:
      - ${ETCD_DATA_0}:/etcd_data
    ports:
      - "${ETCD0_HTTP_PORT}:2379"
      - "${ETCD0_GRPC_PORT}:2380"
    command:
      - /usr/local/bin/etcd
      - -name
      - etcd0
      - --data-dir
      - /etcd_data
      - -advertise-client-urls
      - http://etcd0:2379
      - -listen-client-urls
      - http://0.0.0.0:2379
      - -initial-advertise-peer-urls
      - http://etcd0:2380
      - -listen-peer-urls
      - http://0.0.0.0:2380
      - -initial-cluster
      - etcd0=http://etcd0:2380,etcd1=http://etcd1:2380,etcd2=http://etcd2:2380


  etcd1:
    image: daocloud.io/daocloud/etcd:${ETCD_VERSION}
    container_name: etcd1
    restart: always
    volumes:
      - ${ETCD_DATA_1}:/etcd_data
    ports:
      - "${ETCD1_HTTP_PORT}:2379"
      - "${ETCD1_GRPC_PORT}:2380"
    command:
      - /usr/local/bin/etcd
      - -name
      - etcd1
      - --data-dir
      - /etcd_data
      - -advertise-client-urls
      - http://etcd1:2379
      - -listen-client-urls
      - http://0.0.0.0:2379
      - -initial-advertise-peer-urls
      - http://etcd1:2380
      - -listen-peer-urls
      - http://0.0.0.0:2380
      - -initial-cluster
      - etcd0=http://etcd0:2380,etcd1=http://etcd1:2380,etcd2=http://etcd2:2380

  etcd2:
    image: daocloud.io/daocloud/etcd:${ETCD_VERSION}
    container_name: etcd2
    restart: always
    volumes:
      - ${ETCD_DATA_2}:/etcd_data
    ports:
      - "${ETCD2_HTTP_PORT}:2379"
      - "${ETCD2_GRPC_PORT}:2380"
    command:
      - /usr/local/bin/etcd
      - -name
      - etcd2
      - --data-dir
      - /etcd_data
      - -advertise-client-urls
      - http://etcd2:2379
      - -listen-client-urls
      - http://0.0.0.0:2379
      - -initial-advertise-peer-urls
      - http://etcd2:2380
      - -listen-peer-urls
      - http://0.0.0.0:2380
      - -initial-cluster
      - etcd0=http://etcd0:2380,etcd1=http://etcd1:2380,etcd2=http://etcd2:2380

  # kafka+zk多节点参考 https://github.com/simplesteph/kafka-stack-docker-compose
  #zookeeper
  zoo1:
    image: zookeeper:${ZK_VERSION}
    container_name: zoo1
    hostname: zoo1
    restart: always
    ports:
      - "${ZOOKEEPER_PORT_1}:2181"
    environment:
      ZOO_MY_ID: 1
      ZOO_PORT: ${ZOOKEEPER_PORT_1}
      ZOO_SERVERS: server.1=zoo1:2888:3888
    volumes:
      - ${ZK_DATA_1}:/data
      - ${ZK_DATALOG_1}:/datalog
    networks:
      - default


  #kafka
  kafka1:
    image: confluentinc/cp-kafka:${KAFKA_VERSION}
    container_name: kafka1
    hostname: kafka1
    restart: always
    ports:
      - "${KAFKA_PORT_1}:9092"
    environment:
      KAFKA_HEAP_OPTS: "-Xms256m -Xmx256M"
      KAFKA_ZOOKEEPER_CONNECT: zoo1:2181
      KAFKA_ADVERTISED_LISTENERS: LISTENER_DOCKER_INTERNAL://kafka1:19092,LISTENER_DOCKER_EXTERNAL://${DOCKER_HOST_IP:-kafka1}:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_DOCKER_INTERNAL:PLAINTEXT,LISTENER_DOCKER_EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_DOCKER_INTERNAL
      KAFKA_BROKER_ID: 1
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    volumes:
      - ${KAFKA_DATA_1}:/var/lib/kafka/data
    networks:
      - default

#  kafka-manager:
#    image: sheepkiller/kafka-manager:latest
#    restart: always
#    ports:
#      - "${KAFKAMANAGER_PORT}:9000"
#    environment:
#      ZK_HOSTS: zoo1:2181
#      APPLICATION_SECRET: letmein
#      KM_ARGS: -Djava.net.preferIPv4Stack=true

  app_service:
    build:
      context: .
    container_name: app_service
    restart: always
    volumes:
      - ${ARTICLE_CONF_FILE}:/app/app/service/article/configs/application.toml
      - ${POEMS_CONF_FILE}:/app/app/service/poems/configs/application.toml
      - ${WEBINFO_CONF_FILE}:/app/app/service/webinfo/configs/application.toml
      - ${INTERFACE_CONF_FILE}:/app/app/interface/main/configs/application.toml
    ports:
      - "${ART_GRPC}:8000"
      - "${ART_HTTP}:8001"
      - "${POEMS_GRPC}:8100"
      - "${POEMS_HTTP}:8101"
      - "${WEBINFO_GRPC}:8200"
      - "${WEBINFO_HTTP}:8201"
      - "${INTERFACE_HTTP}:8080"
    environment:
      TZ: "$TZ"
networks:
  default: